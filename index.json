[{"content":"Some work projects (available only in Spanish) # ¿Sobre qué tuitean los senadores mexicanos? ( R, Shiny)\nSimulador de ahorro para el retiro para jóvenes (R, SQLite, Shiny)\nPanorama de los profesionistas en México (Python, R, Shiny)\nEl papel de México en la migración centroamericana (Javascript)\nMapa de denuncia ciudadana (R, Leaflet)\n","date":"24 September 2022","permalink":"/projects/","section":"","summary":"Some work projects (available only in Spanish) # ¿Sobre qué tuitean los senadores mexicanos?","title":""},{"content":"","date":"24 September 2022","permalink":"/tags/catboost/","section":"Tags","summary":"","title":"catboost"},{"content":"","date":"24 September 2022","permalink":"/tags/gradient-boosting/","section":"Tags","summary":"","title":"Gradient boosting"},{"content":"","date":"24 September 2022","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine learning"},{"content":" Data Analyst | Data Scientist | Python | R | Machine Learning I love technology and data visualization. Self-taught programmer and training in progress in machine learning. My personal projects are available on this page.\n","date":"24 September 2022","permalink":"/","section":"Orlando F. Vázquez","summary":"Data Analyst | Data Scientist | Python | R | Machine Learning I love technology and data visualization.","title":"Orlando F. Vázquez"},{"content":" How high were the probabilities of dying from COVID-19 in Mexico during 2020? On the most severe days of the pandemic over 90% The Impact of COVID-19 # The COVID-19 pandemic has had serious consequences throughout the world and Mexico has been no exception. In certain months, the number of deaths caused by this disease pushed the health system to the limit and the effects still persist in many of the country\u0026rsquo;s hospitals. INEGI has reported that of the total deaths registered during 2020 (1,086,743), 18.4% of these (200,256) were due to COVID-19, being the first cause of death in the case of men. This can be seen in more detail by analyzing the database of deaths during 2020 available in the page of the Ministry of Health.\nTo try to give context to the situation experienced in the country during that year, I generated a model from this database that could predict, based on age, gender, place of death, education, date of death, among other variables, what were the probabilities that the cause of death of individuals would have been registered as COVID-19.\nThis model was developed with a gradient boosting algorithm, using the Catboost library, obtaining an F1 Score of .80 after making several adjustments. Taking into account the changes in COVID-19 mortality that have occurred in later years as a consequence of the vaccination efforts carried out worldwide, this is not a model that can be extrapolated to later years. In this sense, it is more an exercise to put into practice knowledge learned in machine learning than a predictive model, and even more detailed results can be found and analyzed if the database made available by the Ministry of Health is analyzed in detail.\nProbability of death due to COVID-19 # What this model can help us with, is to have a better idea of what was the likelihood that a person, in the course of 2020, would have COVID-19 as a cause of death, taking into consideration the sociodemographic characteristics previously mentioned: gender, age, date of death, locality of death, among others. What we find is that the probability of this being the cause of death increases significantly, in concordance with the graph above, which shows that the first wave of COVID-19 in 2020.\nFor example, the chance that a 68-year-old man, in Mexico City, in the municipality of Gustavo A. Madero, with incomplete primary education, on December 22, 2020, would have died as a result of COVID-19 is up to 92%, which coincides with the highest peak of deaths in 2020 shown in the graph above.\nThe result changes radically if we consider the case of a woman of the same age and in the same locality, but who died in the first half of April. In this case, the probability that the cause of death was COVID-19 decreases to 39%. Playing a little more with the conditions of death, very interesting patterns can be found in different regions of the country depending on the selected temporality and demographic conditions.\n","date":"24 September 2022","permalink":"/projects/covid19-prediction/","section":"","summary":"How high were the probabilities of dying from COVID-19 in Mexico during 2020? On the most severe days of the pandemic over 90%","title":"Predicting the probability of death from COVID-19 in Mexico during 2020"},{"content":"","date":"24 September 2022","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"28 August 2022","permalink":"/tags/bloom/","section":"Tags","summary":"","title":"BLOOM"},{"content":"","date":"28 August 2022","permalink":"/tags/lstm/","section":"Tags","summary":"","title":"LSTM"},{"content":"","date":"28 August 2022","permalink":"/tags/rnn/","section":"Tags","summary":"","title":"RNN"},{"content":" On the path followed to generate text in Spanish Spanish text generation # I want to start this text by immediately showing the result of the first project I did in machine learning: a Spanish text generator from fine-tuning an existing model (BLOOM) using Transformers. The training was carried out from the transcripts of the press conferences and public speeches of the president of Mexico, Andrés Manuel López Obrador, so the generated text is characterized by replicating his particular way of communicating verbally.\nFrom here on, the story of the path to reach this result, clarifying that the text is not intended to be a tutorial or to explain the specific details for the creation of this app..\nSetting up the project # In my journey to learn recurrent neural networks (RNN) I decided that one of the first projects I would work on personally would be one of character-level text generation, which would help me to better understand how it works and to put into practice what I had learned. I decided to start the project by working from Donald Trump\u0026rsquo;s texts, considering that there would be a lot of material available, although it soon became evident that it was not exactly the most innovative idea.\nTo give a little more originality and diversity to the subject, I chose to work not from texts written by the former president in books or tweets, but from the transcripts of his public speeches, so we could see this exercise as a speech generator.\nFirst results and change of approach # Using a LSTM (Long Short Term Memory) neural network in PyTorch and with a training of 30 epochs the first results obtained were\u0026hellip; curious:\nThe media is going to be talking about the farmers. They’re a very good job and we are trying to be a great job that they have to be and with the problem. I think it’s not allowed to get it, they don’t want to say it. They want to give them all the time. I think it will start to get along with a land. We had a great service and the first time to say the sand and energy independence, but when the suburbs, they’ve been able to do that\u0026hellip;\nAs an exercise it was frankly entertaining and the result was fun to read. The generated text, although the style of expression of the former US president can be appreciated, lacks coherence. However, in a model trained from scratch, with limited hardware, it was to be expected that the results wouldn\u0026rsquo;t be spectacular on the first try. Having already read some literature on transfer learning it seemed to me that this could be an excellent exercise to put into practice and see the benefits that could be obtained using previously trained models.\nUsing transformers and GPT-2 # The next step in this project was to document myself on the operation of the Transformers library from Hugging Face and try to improve the results previously obtained. The model used this time was GPT-2, a text generation model made available by OpenAI in 2019. After much trial and error and after adjusting several parameters in the model, I managed to generate the following texts:\nMuch higher quality results were observed than in the first exercise and in a relatively short amount of time, which indicated that this was a good way to go.\nModels to generate text in Spanish # However, what I really wanted to do from the beginning was to generate text in Spanish, trying to replicate the quality of the results obtained by training the model in English. Hence, I considered that once acceptable results were obtained in the generation of English text, it would be easy to replicate the results with Spanish text. Unfortunately, this was not the case and some articles already mention the limitations of the GPT-2 model for text generation in languages other than English.\nWhile models of GPT-2 trained with a Spanish corpus are available at *Hugging Face, having recently read about this initiative, I decided it would be a good exercise to test the multilingual model of *BLOOM, an open science and open access project trained to generate text in 46 languages and made public in July of this year.\nWeb scraping # This time I decided that the generation of text in Spanish would be from transcriptions of conferences and speeches of President Andrés Manuel López Obrador, and the reason for selecting this character was similar to the previous one: wide availability of material in the internet. The process to obtain the transcripts was based on web scraping from the [president\u0026rsquo;s] web page (https://lopezobrador.org.mx/transcripciones/) where these transcripts are available.\nUsing Scrapy I obtained 1609 transcripts of press conferences and speeches at public events from December 4, 2018 to July 21, 2022. After cleaning the transcripts and keeping only the participations of President López Obrador and eliminating the interventions of any other character, this resulted in 38 MB of transcribed speeches and conferences, equivalent to 36,795,294 characters.\nModel training and conclusions # In order to train the model and due to hardware limitations I had to make several adjustments: firstly use the smallest BLOOM model available (which still has 560 million parameters), keep only 25% of the text obtained, which in turn, to avoid problems with RAM, I had to divide into complete sentences with a maximum of 1000 characters.\nAnother additional limitation was that when programming the application using Gradio and uploading it to Hugging Spaces, due to the time required to generate the text, I found it necessary to limit the generation to only 100 words.\nThe result of the fine tuning after a training of 5 epochs, is the generation of a decent text, as you can see at the beginning, which unlike the first exercises, maintains at least a coherence and structure within the generated sentences. I consider that there\u0026rsquo;s still room for improvement and fortunately the transformers API has enough flexibility to adjust different parameters both in the training and in the text generation. I hope to upload soon a detailed tutorial about the steps followed in this project.\n","date":"28 August 2022","permalink":"/projects/text-generation/","section":"","summary":"On the path followed to generate text in Spanish","title":"Spanish text generation project"},{"content":"","date":"28 August 2022","permalink":"/tags/transformers/","section":"Tags","summary":"","title":"Transformers"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]